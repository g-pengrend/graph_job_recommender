{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading node embeddings...\n",
      "Loading node embeddings from cache...\n",
      "Node embeddings loaded successfully!\n",
      "Loading/initializing language model...\n",
      "Loading model from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brandon\\AppData\\Local\\Temp\\ipykernel_19364\\1248321860.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  node_embeddings = torch.load(CACHE_PATHS['node_embeddings'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from cache successfully!\n",
      "Using device: cuda\n",
      "Loading graph and dataframe...\n",
      "Graph and dataframe loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import ast\n",
    "from typing import List, Dict, Union, Tuple, Optional\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "import annoy\n",
    "from pathlib import Path\n",
    "import time\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle\n",
    "import faiss\n",
    "from dataclasses import dataclass\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import ollama\n",
    "import re\n",
    "\n",
    "# Initialization of job recommendation system components\n",
    "\n",
    "# Define cache paths\n",
    "CACHE_DIR = Path('recommendation_cache')\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "CACHE_PATHS = {\n",
    "    'graph': CACHE_DIR / 'final_complete_graph.pkl',\n",
    "    'dataframe': CACHE_DIR / 'final_graph_dataframe.pkl',\n",
    "    'annoy_index': CACHE_DIR / 'annoy_index.ann',\n",
    "    'faiss_index': CACHE_DIR / 'faiss_index.pkl', \n",
    "    'graph_metrics': CACHE_DIR / 'graph_metrics.pkl',\n",
    "    'embeddings_norm': CACHE_DIR / 'normalized_embeddings.pkl',\n",
    "    'degree_scores': CACHE_DIR / 'degree_scores.pkl',\n",
    "    'model': CACHE_DIR / 'model_cache.pkl',\n",
    "    'node_embeddings': CACHE_DIR / 'node_embeddings.pt'\n",
    "}\n",
    "\n",
    "# Load node embeddings\n",
    "print(\"Loading node embeddings...\")\n",
    "if CACHE_PATHS['node_embeddings'].exists():\n",
    "    print(\"Loading node embeddings from cache...\")\n",
    "    node_embeddings = torch.load(CACHE_PATHS['node_embeddings'])\n",
    "    print(\"Node embeddings loaded successfully!\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Node embeddings file not found. Please ensure node embeddings have been generated and saved from step 8.\")\n",
    "\n",
    "# MultiLabelBinarizer setup for job type decoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([['contract'], ['fulltime'], ['internship'], ['parttime'], ['temporary']])\n",
    "\n",
    "# Geocoder initialization\n",
    "geolocator = Nominatim(user_agent=\"job_recommender_v1\")\n",
    "\n",
    "# Load or initialize model and tokenizer\n",
    "def load_or_init_model():\n",
    "    print(\"Loading/initializing language model...\")\n",
    "    if CACHE_PATHS['model'].exists():\n",
    "        print(\"Loading model from cache...\")\n",
    "        with open(CACHE_PATHS['model'], 'rb') as f:\n",
    "            cache = pickle.load(f)\n",
    "            tokenizer = cache['tokenizer']\n",
    "            model = cache['model']\n",
    "        print(\"Model loaded from cache successfully!\")\n",
    "    else:\n",
    "        print(\"Downloading and caching model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "        model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "        \n",
    "        with open(CACHE_PATHS['model'], 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'tokenizer': tokenizer,\n",
    "                'model': model\n",
    "            }, f)\n",
    "        print(\"Model downloaded and cached successfully!\")\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "    return tokenizer, model, device\n",
    "\n",
    "# Initialize tokenizer, model, and device\n",
    "tokenizer, model, device = load_or_init_model()\n",
    "\n",
    "# Load graph and dataframe\n",
    "print(\"Loading graph and dataframe...\")\n",
    "if CACHE_PATHS['graph'].exists() and CACHE_PATHS['dataframe'].exists():\n",
    "    with open(CACHE_PATHS['graph'], 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "    with open(CACHE_PATHS['dataframe'], 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "    print(\"Graph and dataframe loaded successfully!\")\n",
    "else:\n",
    "    print(\"Error: Graph or dataframe pickle files do not exist. Please check the paths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "# Function to build Annoy index\n",
    "def build_ann_index(embeddings_np, n_trees=100):\n",
    "    print(\"\\nBuilding/Loading Annoy index...\")\n",
    "    if CACHE_PATHS['annoy_index'].exists():\n",
    "        print(\"Loading cached Annoy index...\")\n",
    "        index = annoy.AnnoyIndex(embeddings_np.shape[1], 'angular')\n",
    "        index.load(str(CACHE_PATHS['annoy_index']))\n",
    "        print(\"Annoy index loaded successfully!\")\n",
    "        return index\n",
    "    \n",
    "    print(\"Building new Annoy index...\")\n",
    "    index = annoy.AnnoyIndex(embeddings_np.shape[1], 'angular')\n",
    "    \n",
    "    for i in range(len(embeddings_np)):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Adding item {i}/{len(embeddings_np)} to Annoy index...\")\n",
    "        index.add_item(i, embeddings_np[i])\n",
    "    \n",
    "    print(\"Building index with trees...\")\n",
    "    index.build(n_trees)\n",
    "    print(\"Saving index to disk...\")\n",
    "    index.save(str(CACHE_PATHS['annoy_index']))\n",
    "    print(\"Annoy index built and saved successfully!\")\n",
    "    return index\n",
    "\n",
    "# Function to build FAISS index\n",
    "def build_faiss_index(embeddings):\n",
    "    print(\"\\nBuilding/Loading FAISS index...\")\n",
    "    if CACHE_PATHS['faiss_index'].exists():\n",
    "        print(\"Loading cached FAISS index...\")\n",
    "        with open(CACHE_PATHS['faiss_index'], 'rb') as f:\n",
    "            index = pickle.load(f)\n",
    "        print(\"FAISS index loaded successfully!\")\n",
    "        return index\n",
    "            \n",
    "    print(\"Building new FAISS index...\")\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    print(\"Normalizing embeddings...\")\n",
    "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1)[:, np.newaxis]\n",
    "    print(\"Adding vectors to index...\")\n",
    "    index.add(normalized_embeddings.astype('float32'))\n",
    "    \n",
    "    print(\"Saving index to disk...\")\n",
    "    with open(CACHE_PATHS['faiss_index'], 'wb') as f:\n",
    "        pickle.dump(index, f)\n",
    "    print(\"FAISS index built and saved successfully!\")\n",
    "    return index\n",
    "\n",
    "# Function to cache normalized embeddings\n",
    "def cache_normalized_embeddings(embeddings_np):\n",
    "    print(\"\\nPreparing normalized embeddings...\")\n",
    "    if CACHE_PATHS['embeddings_norm'].exists():\n",
    "        print(\"Loading cached normalized embeddings...\")\n",
    "        with open(CACHE_PATHS['embeddings_norm'], 'rb') as f:\n",
    "            normalized = pickle.load(f)\n",
    "        print(\"Normalized embeddings loaded successfully!\")\n",
    "        return normalized\n",
    "    \n",
    "    print(\"Computing normalized embeddings...\")\n",
    "    normalized = embeddings_np / np.linalg.norm(embeddings_np, axis=1)[:, np.newaxis]\n",
    "    print(\"Saving normalized embeddings to disk...\")\n",
    "    with open(CACHE_PATHS['embeddings_norm'], 'wb') as f:\n",
    "        pickle.dump(normalized, f)\n",
    "    print(\"Normalized embeddings cached successfully!\")\n",
    "    return normalized\n",
    "\n",
    "def compute_pagerank_torch(graph, damping=0.85, max_iter=100, tol=1e-6):\n",
    "    print(\"\\nComputing PageRank scores...\")\n",
    "    # Create node ID to index mapping\n",
    "    node_map = {node: idx for idx, node in enumerate(graph.nodes())}\n",
    "    reverse_map = {idx: node for node, idx in node_map.items()}\n",
    "    n = len(node_map)\n",
    "    \n",
    "    # Convert edges using the mapping\n",
    "    edges = [(node_map[e[0]], node_map[e[1]]) for e in graph.edges()]\n",
    "    row = np.array([e[0] for e in edges])\n",
    "    col = np.array([e[1] for e in edges])\n",
    "    data = np.ones(len(edges))\n",
    "    adj_matrix = csr_matrix((data, (row, col)), shape=(n, n))\n",
    "\n",
    "    print(\"Normalizing adjacency matrix...\")\n",
    "    out_degree = np.array(adj_matrix.sum(axis=1)).flatten()\n",
    "    out_degree[out_degree == 0] = 1\n",
    "    D_inv = csr_matrix((1.0 / out_degree, (np.arange(n), np.arange(n))), shape=(n, n))\n",
    "    stochastic_matrix = D_inv @ adj_matrix\n",
    "\n",
    "    print(\"Converting to PyTorch sparse format...\")\n",
    "    coo_matrix = stochastic_matrix.tocoo()\n",
    "    indices = torch.tensor([coo_matrix.row, coo_matrix.col], dtype=torch.long)\n",
    "    values = torch.tensor(coo_matrix.data, dtype=torch.float32)\n",
    "    sparse_matrix = torch.sparse.FloatTensor(indices, values, torch.Size([n, n])).cuda()\n",
    "\n",
    "    print(\"Initializing PageRank computation...\")\n",
    "    pagerank_vector = torch.ones(n, device=\"cuda\") / n\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        new_pagerank_vector = (1 - damping) / n + damping * torch.sparse.mm(sparse_matrix.t(), pagerank_vector.unsqueeze(1)).squeeze()\n",
    "        if torch.norm(new_pagerank_vector - pagerank_vector, p=1) < tol:\n",
    "            print(f\"PageRank converged after {i + 1} iterations\")\n",
    "            break\n",
    "        pagerank_vector = new_pagerank_vector\n",
    "\n",
    "    print(\"PageRank computation completed!\")\n",
    "    # Convert back to original node IDs\n",
    "    pagerank_dict = {reverse_map[i]: score for i, score in enumerate(pagerank_vector.cpu().numpy())}\n",
    "    return pagerank_dict\n",
    "\n",
    "# Function to cache graph metrics\n",
    "def cache_graph_metrics(graph):\n",
    "    print(\"\\nComputing/Loading graph metrics...\")\n",
    "    if CACHE_PATHS['graph_metrics'].exists():\n",
    "        print(\"Loading cached graph metrics...\")\n",
    "        with open(CACHE_PATHS['graph_metrics'], 'rb') as f:\n",
    "            metrics = pickle.load(f)\n",
    "        print(\"Graph metrics loaded successfully!\")\n",
    "        return metrics\n",
    "    \n",
    "    print(\"Computing graph metrics...\")\n",
    "    print(\"Computing degree centrality...\")\n",
    "    degree = dict(graph.degree())\n",
    "    max_degree = max(degree.values())\n",
    "    \n",
    "    print(\"Computing PageRank...\")\n",
    "    pagerank = compute_pagerank_torch(graph)\n",
    "    \n",
    "    print(\"Computing core numbers...\")\n",
    "    core_numbers = nx.core_number(graph)\n",
    "    \n",
    "    metrics = {\n",
    "        'degree': degree,\n",
    "        'max_degree': max_degree,\n",
    "        'pagerank': pagerank,\n",
    "        'core_numbers': core_numbers\n",
    "    }\n",
    "    \n",
    "    print(\"Saving graph metrics to disk...\")\n",
    "    with open(CACHE_PATHS['graph_metrics'], 'wb') as f:\n",
    "        pickle.dump(metrics, f)\n",
    "    print(\"Graph metrics cached successfully!\")\n",
    "    return metrics\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def get_job_description_embedding(text, tokenizer, model, max_chunk_length=512, overlap=50, device='cpu'):\n",
    "    print(\"\\nGenerating job description embedding...\")\n",
    "    tokens_per_chunk = max_chunk_length - 2  \n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i+tokens_per_chunk]) for i in range(0, len(words), tokens_per_chunk - overlap)]\n",
    "    \n",
    "    embeddings = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "        encoded_input = tokenizer(chunk, return_tensors='pt', padding=True, truncation=True, max_length=max_chunk_length).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        \n",
    "        sentence_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embedding = F.normalize(sentence_embedding, p=2, dim=1)\n",
    "        embeddings.append(sentence_embedding.cpu().numpy())\n",
    "\n",
    "    if embeddings:\n",
    "        print(\"Averaging chunk embeddings...\")\n",
    "        return np.mean(embeddings, axis=0).squeeze()\n",
    "    else:\n",
    "        print(\"Warning: No embeddings generated, returning zero vector\")\n",
    "        return np.zeros((384,))\n",
    "\n",
    "def get_job_title_embedding(text, tokenizer, model, device='cpu'):\n",
    "    print(f\"\\nGenerating embedding for job title: {text}\")\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    sentence_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    sentence_embedding = F.normalize(sentence_embedding, p=2, dim=1)\n",
    "    print(\"Job title embedding generated successfully!\")\n",
    "    \n",
    "    return sentence_embedding.cpu().numpy().squeeze()\n",
    "\n",
    "def process_job_description_with_LLM(document_text):\n",
    "    \"\"\"Process a single document using the same LLM setup.\"\"\"\n",
    "    model_name = 'capybarahermes-2.5-mistral-7b.Q5_K_M.gguf:latest'\n",
    "    prompt = f\"\"\"You are an expert in understanding job descriptions and extracting the details and even nuanced requirements for the job. Your goal is to read the input slowly and take time to consider what is written, extract the information and break it down into these 3 aspects:\n",
    "    1. responsibilites \n",
    "    2. qualifications\n",
    "    3. skills, technical and non-technical\n",
    "and summarize it in point form line by line.\n",
    "With each aspect answered, ensure that each of the aspects are properly differentiated and avoid overlaps as much as possible.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': prompt},\n",
    "                {'role': 'user', 'content': document_text}\n",
    "            ]\n",
    "        )\n",
    "        response_text = response['message']['content']\n",
    "        \n",
    "        # Clean the response text\n",
    "        # Remove special characters except alphanumeric, spaces, periods and commas\n",
    "        cleaned_text = re.sub(r'[^A-Za-z0-9\\s.,]', '', response_text)\n",
    "        # Remove point formatted numbers but keep time patterns\n",
    "        cleaned_text = re.sub(r'(?<!\\d)(\\d+)\\.(?!\\d)', '', cleaned_text).strip()\n",
    "        \n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {e}\")\n",
    "        return None\n",
    "\n",
    "@dataclass\n",
    "class UserPreferences:\n",
    "    \"\"\"User preferences for job recommendations\"\"\"\n",
    "    location: Optional[Tuple[float, float]] = None\n",
    "    location_name: Optional[str] = None  # Added for geocoding\n",
    "    job_title: Optional[str] = None\n",
    "    job_description: Optional[str] = None\n",
    "    max_distance_km: float = 10.0\n",
    "    weights: dict = None\n",
    "    remote_preference: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.weights is None:\n",
    "            self.weights = {\n",
    "                'title_similarity': 0.3,\n",
    "                'description_similarity': 0.2,\n",
    "                'location_proximity': 0.2,\n",
    "                'degree': 0.1,\n",
    "                'pagerank': 0.1,\n",
    "                'core_number': 0.1\n",
    "            }\n",
    "def get_graph_based_recommendations_v2(\n",
    "    graph: nx.Graph,\n",
    "    node_embeddings: torch.Tensor,\n",
    "    preferences: UserPreferences,\n",
    "    n_hops: int = 2,\n",
    "    top_k: int = 5,\n",
    "    n_candidates: int = 1000\n",
    ") -> List[dict]:\n",
    "    \"\"\"Generate recommendations using both FAISS and Annoy for hybrid search\"\"\"\n",
    "    print(\"\\nStarting recommendation generation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Preparing embeddings...\")\n",
    "    embeddings_np = node_embeddings.numpy()\n",
    "    print(f\"Embeddings shape: {embeddings_np.shape}\")\n",
    "    \n",
    "    print(\"\\nInitializing search indices...\")\n",
    "    faiss_index = build_faiss_index(embeddings_np)\n",
    "    annoy_index = build_ann_index(embeddings_np)\n",
    "    normalized_embeddings = cache_normalized_embeddings(embeddings_np)\n",
    "    graph_metrics = cache_graph_metrics(graph)\n",
    "    \n",
    "    # Create mapping from string ID to graph node name\n",
    "    id_to_node = {row['id']: f\"job_{idx}\" for idx, row in df.iterrows()}\n",
    "    node_to_id = {f\"job_{idx}\": row['id'] for idx, row in df.iterrows()}\n",
    "    \n",
    "    print(\"\\nGenerating candidate nodes...\")\n",
    "    candidate_indices = set()\n",
    "    if preferences.job_title:\n",
    "        print(f\"Finding similar jobs to title: {preferences.job_title}\")\n",
    "        title_embedding = get_job_title_embedding(preferences.job_title, tokenizer, model, device)\n",
    "        title_embedding = title_embedding / np.linalg.norm(title_embedding)\n",
    "        \n",
    "        print(\"Searching with FAISS...\")\n",
    "        D_faiss, I_faiss = faiss_index.search(title_embedding.astype('float32').reshape(1,-1), top_k)\n",
    "        faiss_candidates = set(I_faiss[0])\n",
    "        \n",
    "        print(\"Searching with Annoy...\")\n",
    "        annoy_candidates = set(annoy_index.get_nns_by_vector(\n",
    "            title_embedding, \n",
    "            top_k,\n",
    "            search_k=-1\n",
    "        ))\n",
    "        \n",
    "        candidate_indices = faiss_candidates.union(annoy_candidates)\n",
    "        print(f\"Found {len(candidate_indices)} candidate indices\")\n",
    "    else:\n",
    "        print(\"No job title specified, using PageRank for candidate selection...\")\n",
    "        weights = np.array([graph_metrics['pagerank'][f\"job_{i}\"] for i in range(len(df))])\n",
    "        weights = weights / weights.sum()\n",
    "        candidate_indices = set(np.random.choice(\n",
    "            len(df),\n",
    "            size=min(n_candidates, len(df)),\n",
    "            p=weights,\n",
    "            replace=False\n",
    "        ))\n",
    "    \n",
    "    print(\"\\nComputing job scores...\")\n",
    "    \n",
    "    # Determine which inputs are available\n",
    "    has_title = preferences.job_title is not None\n",
    "    has_description = preferences.job_description is not None\n",
    "    has_location = preferences.location is not None\n",
    "    \n",
    "    # Set weights based on available inputs\n",
    "    if has_title and has_description:\n",
    "        # All semantic inputs available\n",
    "        weights = {\n",
    "            'title_similarity': 0.25,\n",
    "            'description_similarity': 0.25,\n",
    "            'pagerank': 0.15,\n",
    "            'degree': 0.10,\n",
    "            'core_number': 0.05,\n",
    "            'location_proximity': 0.20 if has_location else 0.0\n",
    "        }\n",
    "    elif has_title or has_description:\n",
    "        # Only one semantic input\n",
    "        semantic_weight = 0.40\n",
    "        graph_weight_total = 0.40\n",
    "        weights = {\n",
    "            'title_similarity': semantic_weight if has_title else 0.0,\n",
    "            'description_similarity': semantic_weight if has_description else 0.0,\n",
    "            'pagerank': 0.20,\n",
    "            'degree': 0.15,\n",
    "            'core_number': 0.05,\n",
    "            'location_proximity': 0.20 if has_location else 0.0\n",
    "        }\n",
    "    else:\n",
    "        # No semantic inputs\n",
    "        weights = {\n",
    "            'pagerank': 0.35,\n",
    "            'degree': 0.35,\n",
    "            'core_number': 0.10,\n",
    "            'location_proximity': 0.20 if has_location else 0.0\n",
    "        }\n",
    "    \n",
    "    # Normalize weights if location is not provided\n",
    "    if not has_location:\n",
    "        total_weight = sum(weights.values())\n",
    "        weights = {k: v/total_weight for k, v in weights.items()}\n",
    "    \n",
    "    # Update preferences with new weights\n",
    "    preferences.weights = weights\n",
    "    \n",
    "    # Pre-compute normalized graph metrics\n",
    "    print(\"Normalizing graph metrics...\")\n",
    "    max_pagerank = max(graph_metrics['pagerank'].values())\n",
    "    max_core_number = max(graph_metrics['core_numbers'].values())\n",
    "    \n",
    "    # Normalize degree scores for the subgraph\n",
    "    subgraph = graph.subgraph([f\"job_{i}\" for i in candidate_indices])\n",
    "    degree_scores = dict(subgraph.degree())\n",
    "    max_degree = max(degree_scores.values()) if degree_scores else 1\n",
    "    \n",
    "    batch_size = 100\n",
    "    job_scores = []\n",
    "    total_batches = len(candidate_indices) // batch_size + (1 if len(candidate_indices) % batch_size else 0)\n",
    "    \n",
    "    for i in range(0, len(candidate_indices), batch_size):\n",
    "        batch_indices = list(candidate_indices)[i:i + batch_size]\n",
    "        current_batch = i // batch_size + 1\n",
    "        print(f\"\\nProcessing batch {current_batch}/{total_batches}...\")\n",
    "        batch_scores = []\n",
    "        \n",
    "        for idx in batch_indices:\n",
    "            try:\n",
    "                node = f\"job_{idx}\"\n",
    "                attrs = graph.nodes[node]\n",
    "                \n",
    "                score_components = {}\n",
    "                \n",
    "                # Normalize semantic similarities by mapping from [-1,1] to [0,1]\n",
    "                if preferences.job_title:\n",
    "                    raw_similarity = np.dot(\n",
    "                        title_embedding,\n",
    "                        normalized_embeddings[idx]\n",
    "                    )\n",
    "                    # Map from [-1,1] to [0,1] using (x + 1) / 2\n",
    "                    score_components['title_similarity'] = (raw_similarity + 1) / 2\n",
    "\n",
    "                if preferences.job_description:\n",
    "                    desc_embedding = get_job_description_embedding(\n",
    "                        preferences.job_description,\n",
    "                        tokenizer,\n",
    "                        model,\n",
    "                        device=device\n",
    "                    )\n",
    "                    raw_similarity = np.dot(\n",
    "                        desc_embedding,\n",
    "                        normalized_embeddings[idx]\n",
    "                    )\n",
    "                    # Map from [-1,1] to [0,1] using (x + 1) / 2\n",
    "                    score_components['description_similarity'] = (raw_similarity + 1) / 2\n",
    "                \n",
    "                # Normalize location proximity using exponential decay\n",
    "                if preferences.location:\n",
    "                    job_location = (\n",
    "                        ast.literal_eval(attrs['lat_long'])\n",
    "                        if isinstance(attrs['lat_long'], str)\n",
    "                        else attrs['lat_long']\n",
    "                    )\n",
    "                    distance = geodesic(preferences.location, job_location).kilometers\n",
    "                    score_components['location_proximity'] = np.exp(-distance / preferences.max_distance_km)\n",
    "                \n",
    "                # Normalize graph metrics\n",
    "                score_components['degree'] = degree_scores[node] / max_degree\n",
    "                score_components['pagerank'] = graph_metrics['pagerank'][node] / max_pagerank\n",
    "                score_components['core_number'] = graph_metrics['core_numbers'][node] / max_core_number\n",
    "                \n",
    "                # Verify all components are in [0,1] range\n",
    "                for component in score_components:\n",
    "                    score_components[component] = max(0, min(1, score_components[component]))\n",
    "                \n",
    "                # Calculate final score using normalized components and weights\n",
    "                final_score = sum(\n",
    "                    score * preferences.weights.get(component, 0)\n",
    "                    for component, score in score_components.items()\n",
    "                )\n",
    "                \n",
    "                job_type_encoded = attrs['job_type_encoding']\n",
    "                if isinstance(job_type_encoded, str):\n",
    "                    job_type_encoded = ast.literal_eval(job_type_encoded)\n",
    "                job_type_encoded = np.array(job_type_encoded)\n",
    "                if job_type_encoded.ndim == 1:\n",
    "                    job_type_encoded = job_type_encoded.reshape(1, -1)\n",
    "\n",
    "                job_type_decoded = mlb.inverse_transform(job_type_encoded)[0]\n",
    "\n",
    "                batch_scores.append({\n",
    "                    'index': node_to_id[node],\n",
    "                    'company': attrs['company'],\n",
    "                    'job_type': job_type_decoded,\n",
    "                    'location': attrs['lat_long'],\n",
    "                    'is_remote': attrs['is_remote'],\n",
    "                    'score_components': score_components,\n",
    "                    'final_score': final_score\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing index {idx}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        job_scores.extend(batch_scores)\n",
    "    \n",
    "    print(\"\\nSorting recommendations...\")\n",
    "    recommendations = sorted(\n",
    "        job_scores,\n",
    "        key=lambda x: x['final_score'],\n",
    "        reverse=True\n",
    "    )[:top_k]\n",
    "    \n",
    "    # Get URLs and titles for only the top recommendations\n",
    "    for rec in recommendations:\n",
    "        job_data = df.loc[df['id'] == rec['index']].iloc[0]\n",
    "        rec['job_url'] = job_data['job_url']\n",
    "        rec['job_url_direct'] = job_data['job_url_direct']\n",
    "        rec['title'] = job_data['title']\n",
    "    \n",
    "    print(f\"\\nRecommendation generation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    return recommendations\n",
    "\n",
    "def get_graph_based_recommendations_v1(\n",
    "    graph: nx.Graph,\n",
    "    node_embeddings: torch.Tensor,\n",
    "    preferences: UserPreferences,\n",
    "    n_hops: int = 2,\n",
    "    top_k: int = 5,\n",
    "    n_candidates: int = 1000\n",
    ") -> List[dict]:\n",
    "    \"\"\"Generate recommendations using both FAISS and Annoy for hybrid search\"\"\"\n",
    "    print(\"\\nStarting recommendation generation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Preparing embeddings...\")\n",
    "    embeddings_np = node_embeddings.numpy()\n",
    "    print(f\"Embeddings shape: {embeddings_np.shape}\")\n",
    "    \n",
    "    print(\"\\nInitializing search indices...\")\n",
    "    faiss_index = build_faiss_index(embeddings_np)\n",
    "    annoy_index = build_ann_index(embeddings_np)\n",
    "    normalized_embeddings = cache_normalized_embeddings(embeddings_np)\n",
    "    graph_metrics = cache_graph_metrics(graph)\n",
    "    \n",
    "    # Create mapping from string ID to graph node name\n",
    "    id_to_node = {row['id']: f\"job_{idx}\" for idx, row in df.iterrows()}\n",
    "    node_to_id = {f\"job_{idx}\": row['id'] for idx, row in df.iterrows()}\n",
    "    \n",
    "    print(\"\\nGenerating candidate nodes...\")\n",
    "    candidate_indices = set()\n",
    "    if preferences.job_title:\n",
    "        print(f\"Finding similar jobs to title: {preferences.job_title}\")\n",
    "        title_embedding = get_job_title_embedding(preferences.job_title, tokenizer, model, device)\n",
    "        title_embedding = title_embedding / np.linalg.norm(title_embedding)\n",
    "        \n",
    "        print(\"Searching with FAISS...\")\n",
    "        D_faiss, I_faiss = faiss_index.search(title_embedding.astype('float32').reshape(1,-1), top_k)\n",
    "        faiss_candidates = set(I_faiss[0])\n",
    "        \n",
    "        print(\"Searching with Annoy...\")\n",
    "        annoy_candidates = set(annoy_index.get_nns_by_vector(\n",
    "            title_embedding, \n",
    "            top_k,\n",
    "            search_k=-1\n",
    "        ))\n",
    "        \n",
    "        candidate_indices = faiss_candidates.union(annoy_candidates)\n",
    "        print(f\"Found {len(candidate_indices)} candidate indices\")\n",
    "    else:\n",
    "        print(\"No job title specified, using PageRank for candidate selection...\")\n",
    "        weights = np.array([graph_metrics['pagerank'][f\"job_{i}\"] for i in range(len(df))])\n",
    "        weights = weights / weights.sum()\n",
    "        candidate_indices = set(np.random.choice(\n",
    "            len(df),\n",
    "            size=min(n_candidates, len(df)),\n",
    "            p=weights,\n",
    "            replace=False\n",
    "        ))\n",
    "    \n",
    "    print(\"\\nComputing job scores...\")\n",
    "    subgraph = graph.subgraph([f\"job_{i}\" for i in candidate_indices])\n",
    "    degree_scores = dict(subgraph.degree())\n",
    "    max_degree = max(degree_scores.values()) if degree_scores else 1\n",
    "    \n",
    "    batch_size = 100\n",
    "    job_scores = []\n",
    "    total_batches = len(candidate_indices) // batch_size + (1 if len(candidate_indices) % batch_size else 0)\n",
    "    \n",
    "    for i in range(0, len(candidate_indices), batch_size):\n",
    "        batch_indices = list(candidate_indices)[i:i + batch_size]\n",
    "        current_batch = i // batch_size + 1\n",
    "        print(f\"\\nProcessing batch {current_batch}/{total_batches}...\")\n",
    "        batch_scores = []\n",
    "        \n",
    "        for idx in batch_indices:\n",
    "            try:\n",
    "                node = f\"job_{idx}\"\n",
    "                attrs = graph.nodes[node]\n",
    "                \n",
    "                score_components = {}\n",
    "                \n",
    "                if preferences.job_title:\n",
    "                    score_components['title_similarity'] = np.dot(\n",
    "                        title_embedding,\n",
    "                        normalized_embeddings[idx]\n",
    "                    )\n",
    "                if preferences.job_description:\n",
    "                    desc_embedding = get_job_description_embedding(\n",
    "                        preferences.job_description,\n",
    "                        tokenizer,\n",
    "                        model,\n",
    "                        device=device\n",
    "                    )\n",
    "                    score_components['description_similarity'] = np.dot(\n",
    "                        desc_embedding,\n",
    "                        normalized_embeddings[idx]\n",
    "                    )\n",
    "                if preferences.location:\n",
    "                    job_location = (\n",
    "                        ast.literal_eval(attrs['lat_long'])\n",
    "                        if isinstance(attrs['lat_long'], str)\n",
    "                        else attrs['lat_long']\n",
    "                    )\n",
    "                    distance = geodesic(preferences.location, job_location).kilometers\n",
    "                    # Use exponential decay for location scoring to penalize distance more aggressively\n",
    "                    score_components['location_proximity'] = np.exp(-distance / preferences.max_distance_km)\n",
    "                \n",
    "                score_components['degree'] = degree_scores[node] / max_degree\n",
    "                score_components['pagerank'] = graph_metrics['pagerank'][node]\n",
    "                score_components['core_number'] = graph_metrics['core_numbers'][node] / max(graph_metrics['core_numbers'].values())\n",
    "                \n",
    "                final_score = sum(\n",
    "                    score * preferences.weights.get(component, 0.1)\n",
    "                    for component, score in score_components.items()\n",
    "                )\n",
    "                \n",
    "                job_type_encoded = attrs['job_type_encoding']\n",
    "                if isinstance(job_type_encoded, str):\n",
    "                    job_type_encoded = ast.literal_eval(job_type_encoded)\n",
    "                job_type_encoded = np.array(job_type_encoded)\n",
    "                if job_type_encoded.ndim == 1:\n",
    "                    job_type_encoded = job_type_encoded.reshape(1, -1)\n",
    "\n",
    "                job_type_decoded = mlb.inverse_transform(job_type_encoded)[0]\n",
    "\n",
    "                batch_scores.append({\n",
    "                    'index': node_to_id[node],  # Use the string ID instead of index\n",
    "                    'company': attrs['company'],\n",
    "                    'job_type': job_type_decoded,\n",
    "                    'location': attrs['lat_long'],\n",
    "                    'is_remote': attrs['is_remote'],\n",
    "                    'score_components': score_components,\n",
    "                    'final_score': final_score\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing index {idx}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        job_scores.extend(batch_scores)\n",
    "    \n",
    "    print(\"\\nSorting recommendations...\")\n",
    "    recommendations = sorted(\n",
    "        job_scores,\n",
    "        key=lambda x: x['final_score'],\n",
    "        reverse=True\n",
    "    )[:top_k]\n",
    "    \n",
    "    # Get URLs and titles for only the top recommendations\n",
    "    for rec in recommendations:\n",
    "        job_data = df.loc[df['id'] == rec['index']].iloc[0]  # Use string ID to lookup\n",
    "        rec['job_url'] = job_data['job_url']\n",
    "        rec['job_url_direct'] = job_data['job_url_direct']\n",
    "        rec['title'] = job_data['title']\n",
    "    \n",
    "    print(f\"\\nRecommendation generation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    return recommendations\n",
    "\n",
    "def get_personalized_recommendations(version=1, preload_preferences=False):\n",
    "    \"\"\"Interactive function to get user preferences and return recommendations\"\"\"\n",
    "    print(\"\\nJob Recommendation System\")\n",
    "    print(\"------------------------\")\n",
    "    \n",
    "    preferences = UserPreferences()\n",
    "    \n",
    "    if preload_preferences:\n",
    "        # Preload default preferences\n",
    "        preferences.location = (1.3521, 103.8198)  # Singapore coordinates\n",
    "        preferences.location_name = \"Singapore\"\n",
    "        preferences.max_distance_km = 10\n",
    "        preferences.job_title = \"Service Engineer\"\n",
    "        preferences.job_description = \"Skilled in mechanical work, always hardworking, able to handle odd hours, able to do all jobs\"\n",
    "        preferences.weights = {\n",
    "            'title_similarity': 0.3,\n",
    "            'description_similarity': 0.2,\n",
    "            'location_proximity': 0.2,\n",
    "            'degree': 0.1,\n",
    "            'pagerank': 0.1,\n",
    "            'core_number': 0.1\n",
    "        }\n",
    "        print(\"Using preloaded preferences:\")\n",
    "        print(f\"Location: {preferences.location_name}\")\n",
    "        print(f\"Job Title: {preferences.job_title}\")\n",
    "        print(f\"Job Description: {preferences.job_description}\")\n",
    "        print(f\"Max Distance: {preferences.max_distance_km} km\")\n",
    "        print(\"Weights:\", preferences.weights)\n",
    "    else:\n",
    "        use_location = input(\"Would you like to specify a location? (y/n): \").lower() == 'y'\n",
    "        if use_location:\n",
    "            location_input = int(input(\"Enter Postal Code (e.g., 123456): \"))\n",
    "            try:\n",
    "                print(\"Geocoding location...\")\n",
    "                location = geolocator.geocode(location_input)\n",
    "                if location:\n",
    "                    preferences.location = (location.latitude, location.longitude)\n",
    "                    preferences.location_name = location_input\n",
    "                    print(f\"Location found: {location.address}\")\n",
    "                else:\n",
    "                    print(\"Location not found. Please enter coordinates manually.\")\n",
    "                    lat = float(input(\"Enter latitude (e.g., 1.3521 for Singapore): \"))\n",
    "                    lon = float(input(\"Enter longitude (e.g., 103.8198): \"))\n",
    "                    preferences.location = (lat, lon)\n",
    "            except Exception as e:\n",
    "                print(f\"Error geocoding location: {str(e)}\")\n",
    "                print(\"Please enter coordinates manually.\")\n",
    "                lat = float(input(\"Enter latitude (e.g., 1.3521 for Singapore): \"))\n",
    "                lon = float(input(\"Enter longitude (e.g., 103.8198): \"))\n",
    "                preferences.location = (lat, lon)\n",
    "            \n",
    "            preferences.max_distance_km = float(input(\"Maximum distance in km (default 10): \") or 10)\n",
    "        \n",
    "        use_title = input(\"Would you like to specify a job title? (y/n): \").lower() == 'y'\n",
    "        if use_title:\n",
    "            preferences.job_title = input(\"Enter job title: \")\n",
    "        \n",
    "        use_desc = input(\"Would you like to specify a job description? (y/n): \").lower() == 'y'\n",
    "        if use_desc:\n",
    "            preferences.job_description = input(\"Enter job description: \")\n",
    "            use_llm = input(\"Would you like to summarize your job description using our in-built AI matching tool? (y/n): \").lower() == 'y'\n",
    "            if use_llm:\n",
    "                print(\"Processing job description with JD_Matching_Tool...\")\n",
    "                summarized_desc = process_job_description_with_LLM(preferences.job_description)\n",
    "                if summarized_desc:\n",
    "                    print(\"\\nSummarized job description:\\n\")\n",
    "                    print(summarized_desc)\n",
    "                    preferences.job_description = summarized_desc\n",
    "        \n",
    "        print(\"\\nSet the importance for each of the following factors on a scale from 0 (not important) to 1 (very important):\")\n",
    "        if use_title:\n",
    "            preferences.weights['title_similarity'] = float(input(\"How important is it for the job title to match your preferences? (default 0.3): \") or 0.3)\n",
    "        if use_desc:\n",
    "            preferences.weights['description_similarity'] = float(input(\"How important is it for the job description to match your skills and experience? (default 0.2): \") or 0.2)\n",
    "        if use_location:\n",
    "            preferences.weights['location_proximity'] = float(input(\"How important is it for the job to be near your preferred location? (default 0.2): \") or 0.2)\n",
    "\n",
    "        preferences.weights['degree'] = float(input(\"How important is it for the job to be popular or well-connected within the network? (default 0.1): \") or 0.1)\n",
    "        preferences.weights['pagerank'] = float(input(\"How important is it for the job to be influential within the network? (default 0.1): \") or 0.1)\n",
    "        preferences.weights['core_number'] = float(input(\"How important is it for the job to be well-connected within its area or community? (default 0.1): \") or 0.1)\n",
    "    \n",
    "    print(\"\\nGenerating recommendations...\")\n",
    "    if version == 1:\n",
    "        recommendations = get_graph_based_recommendations_v1(\n",
    "            graph=graph,\n",
    "            node_embeddings=node_embeddings,\n",
    "            preferences=preferences\n",
    "        )\n",
    "    elif version == 2:\n",
    "        recommendations = get_graph_based_recommendations_v2(\n",
    "            graph=graph,\n",
    "            node_embeddings=node_embeddings,\n",
    "            preferences=preferences\n",
    "        )        \n",
    "    \n",
    "    print(\"\\nTop Recommendations:\")\n",
    "    print(\"-------------------\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"\\n{i}. Company: {rec['company']}\")\n",
    "        print(f\"   Job Title: {rec['title']}\")\n",
    "        print(f\"   Job Type: {', '.join(rec['job_type'])}\")\n",
    "        if preferences.location:\n",
    "            job_loc = ast.literal_eval(rec['location']) if isinstance(rec['location'], str) else rec['location']\n",
    "            distance = geodesic(preferences.location, job_loc).kilometers\n",
    "            print(f\"   Distance: {distance:.1f} km\")\n",
    "        print(f\"   Remote: {rec['is_remote']}\")\n",
    "        print(f\"   Job URL: {rec['job_url']}\")\n",
    "        print(f\"   Direct URL: {rec['job_url_direct']}\")\n",
    "        print(\"   Scores:\")\n",
    "        for component, score in rec['score_components'].items():\n",
    "            print(f\"      - {component}: {score:.3f}\")\n",
    "        print(f\"   Final Score: {rec['final_score']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Recommendation System\n",
      "------------------------\n",
      "Using preloaded preferences:\n",
      "Location: Singapore\n",
      "Job Title: Service Engineer\n",
      "Job Description: Skilled in mechanical work, always hardworking, able to handle odd hours, able to do all jobs\n",
      "Max Distance: 10 km\n",
      "Weights: {'title_similarity': 0.3, 'description_similarity': 0.2, 'location_proximity': 0.2, 'degree': 0.1, 'pagerank': 0.1, 'core_number': 0.1}\n",
      "\n",
      "Generating recommendations...\n",
      "\n",
      "Starting recommendation generation...\n",
      "Preparing embeddings...\n",
      "Embeddings shape: (25610, 384)\n",
      "\n",
      "Initializing search indices...\n",
      "\n",
      "Building/Loading FAISS index...\n",
      "Loading cached FAISS index...\n",
      "FAISS index loaded successfully!\n",
      "\n",
      "Building/Loading Annoy index...\n",
      "Loading cached Annoy index...\n",
      "Annoy index loaded successfully!\n",
      "\n",
      "Preparing normalized embeddings...\n",
      "Loading cached normalized embeddings...\n",
      "Normalized embeddings loaded successfully!\n",
      "\n",
      "Computing/Loading graph metrics...\n",
      "Loading cached graph metrics...\n",
      "Graph metrics loaded successfully!\n",
      "\n",
      "Generating candidate nodes...\n",
      "Finding similar jobs to title: Service Engineer\n",
      "\n",
      "Generating embedding for job title: Service Engineer\n",
      "Job title embedding generated successfully!\n",
      "Searching with FAISS...\n",
      "Searching with Annoy...\n",
      "Found 10 candidate indices\n",
      "\n",
      "Computing job scores...\n",
      "\n",
      "Processing batch 1/1...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Sorting recommendations...\n",
      "\n",
      "Recommendation generation completed in 1.17 seconds\n",
      "\n",
      "Top Recommendations:\n",
      "-------------------\n",
      "\n",
      "1. Company: Tana Development (Singapore) Pte Ltd\n",
      "   Job Title: Part / Full Time Cook\n",
      "   Job Type: fulltime, parttime\n",
      "   Distance: 6.0 km\n",
      "   Remote: False\n",
      "   Job URL: https://sg.indeed.com/viewjob?jk=00b023a857ba861e\n",
      "   Direct URL: https://www.mycareersfuture.gov.sg/job/food-and-beverage/part-full-time-cook-tana-development-f8b79371ac1cd9b8fdd3a37417bc295b\n",
      "   Scores:\n",
      "      - title_similarity: 0.058\n",
      "      - description_similarity: -0.030\n",
      "      - location_proximity: 0.551\n",
      "      - degree: 1.000\n",
      "      - pagerank: 0.000\n",
      "      - core_number: 0.286\n",
      "   Final Score: 0.250\n",
      "\n",
      "2. Company: LIFEUP MART PTE. LTD.\n",
      "   Job Title: Accountant\n",
      "   Job Type: fulltime\n",
      "   Distance: 6.3 km\n",
      "   Remote: False\n",
      "   Job URL: https://sg.indeed.com/viewjob?jk=01d903fc0a206d1f\n",
      "   Direct URL: https://www.mycareersfuture.gov.sg/job/accounting/accountant-lifeup-mart-b53b2a2c078d103e26ffb9b0255c4e6f\n",
      "   Scores:\n",
      "      - title_similarity: 0.058\n",
      "      - description_similarity: -0.030\n",
      "      - location_proximity: 0.533\n",
      "      - degree: 1.000\n",
      "      - pagerank: 0.000\n",
      "      - core_number: 0.315\n",
      "   Final Score: 0.249\n",
      "\n",
      "3. Company: Singapore Manufacturing Federation\n",
      "   Job Title: Snr Executive/ Executive, Quality and Safety Standards\n",
      "   Job Type: fulltime\n",
      "   Distance: 8.6 km\n",
      "   Remote: False\n",
      "   Job URL: https://sg.indeed.com/viewjob?jk=01e2366962e4c99b\n",
      "   Direct URL: https://www.mycareersfuture.gov.sg/job/admin/snr-executive-executive-quality-safety-standards-singapore-manufacturing-federation-c9006651ac180c0d5000556360e6fcc1\n",
      "   Scores:\n",
      "      - title_similarity: 0.058\n",
      "      - description_similarity: -0.030\n",
      "      - location_proximity: 0.424\n",
      "      - degree: 1.000\n",
      "      - pagerank: 0.000\n",
      "      - core_number: 0.108\n",
      "   Final Score: 0.207\n",
      "\n",
      "4. Company: Oomph Pte. Ltd.\n",
      "   Job Title: Sales Promoter\n",
      "   Job Type: fulltime\n",
      "   Distance: 8.8 km\n",
      "   Remote: False\n",
      "   Job URL: https://sg.indeed.com/viewjob?jk=000a01db7a6ccc16\n",
      "   Direct URL: https://www.mycareersfuture.gov.sg/job/customer-service/sales-promoter-oomph-e63161cdb7dda2c74dfa04a764ed31af\n",
      "   Scores:\n",
      "      - title_similarity: 0.058\n",
      "      - description_similarity: -0.030\n",
      "      - location_proximity: 0.415\n",
      "      - degree: 1.000\n",
      "      - pagerank: 0.000\n",
      "      - core_number: 0.108\n",
      "   Final Score: 0.205\n",
      "\n",
      "5. Company: PHOENIX OPCO PTE. LTD.\n",
      "   Job Title: Porter\n",
      "   Job Type: fulltime\n",
      "   Distance: 9.0 km\n",
      "   Remote: False\n",
      "   Job URL: https://sg.indeed.com/viewjob?jk=0005a77c5af02f32\n",
      "   Direct URL: https://www.mycareersfuture.gov.sg/job/customer-service/porter-phoenix-opco-c6a4a2faf33919b8d6287a84e39748f9\n",
      "   Scores:\n",
      "      - title_similarity: 0.058\n",
      "      - description_similarity: -0.030\n",
      "      - location_proximity: 0.407\n",
      "      - degree: 0.000\n",
      "      - pagerank: 0.000\n",
      "      - core_number: 0.894\n",
      "   Final Score: 0.182\n"
     ]
    }
   ],
   "source": [
    "get_personalized_recommendations(1, preload_preferences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Recommendation System\n",
      "------------------------\n",
      "Using preloaded preferences:\n",
      "Location: Singapore\n",
      "Job Title: Service Engineer\n",
      "Job Description: Skilled in mechanical work, always hardworking, able to handle odd hours, able to do all jobs\n",
      "Max Distance: 10 km\n",
      "Weights: {'title_similarity': 0.3, 'description_similarity': 0.2, 'location_proximity': 0.2, 'degree': 0.1, 'pagerank': 0.1, 'core_number': 0.1}\n",
      "\n",
      "Generating recommendations...\n",
      "\n",
      "Starting recommendation generation...\n",
      "Preparing embeddings...\n",
      "Embeddings shape: (25610, 384)\n",
      "\n",
      "Initializing search indices...\n",
      "\n",
      "Building/Loading FAISS index...\n",
      "Loading cached FAISS index...\n",
      "FAISS index loaded successfully!\n",
      "\n",
      "Building/Loading Annoy index...\n",
      "Loading cached Annoy index...\n",
      "Annoy index loaded successfully!\n",
      "\n",
      "Preparing normalized embeddings...\n",
      "Loading cached normalized embeddings...\n",
      "Normalized embeddings loaded successfully!\n",
      "\n",
      "Computing/Loading graph metrics...\n",
      "Loading cached graph metrics...\n",
      "Graph metrics loaded successfully!\n",
      "\n",
      "Generating candidate nodes...\n",
      "Finding similar jobs to title: Service Engineer\n",
      "\n",
      "Generating embedding for job title: Service Engineer\n",
      "Job title embedding generated successfully!\n",
      "Searching with FAISS...\n",
      "Searching with Annoy...\n",
      "Found 10 candidate indices\n",
      "\n",
      "Computing job scores...\n",
      "Normalizing graph metrics...\n",
      "\n",
      "Processing batch 1/1...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Generating job description embedding...\n",
      "Processing chunk 1/1...\n",
      "Averaging chunk embeddings...\n",
      "\n",
      "Sorting recommendations...\n",
      "\n",
      "Recommendation generation completed in 1.04 seconds\n",
      "\n",
      "Top Recommendations:\n",
      "-------------------\n",
      "\n",
      "1. Company: Tana Development (Singapore) Pte Ltd\n",
      "   Job Title: Part / Full Time Cook\n",
      "   Job Type: fulltime, parttime\n",
      "   Distance: 6.0 km\n",
      "   Remote: False\n",
      "   Job URL: https://sg.indeed.com/viewjob?jk=00b023a857ba861e\n",
      "   Direct URL: https://www.mycareersfuture.gov.sg/job/food-and-beverage/part-full-time-cook-tana-development-f8b79371ac1cd9b8fdd3a37417bc295b\n",
      "   Scores:\n",
      "      - title_similarity: 0.529\n",
      "      - description_similarity: 0.485\n",
      "      - location_proximity: 0.551\n",
      "      - degree: 1.000\n",
      "      - pagerank: 0.000\n",
      "      - core_number: 0.286\n",
      "   Final Score: 0.478\n",
      "\n",
      "2. Company: LIFEUP MART PTE. LTD.\n",
      "   Job Title: Accountant\n",
      "   Job Type: fulltime\n",
      "   Distance: 6.3 km\n",
      "   Remote: False\n",
      "   Job URL: https://sg.indeed.com/viewjob?jk=01d903fc0a206d1f\n",
      "   Direct URL: https://www.mycareersfuture.gov.sg/job/accounting/accountant-lifeup-mart-b53b2a2c078d103e26ffb9b0255c4e6f\n",
      "   Scores:\n",
      "      - title_similarity: 0.529\n",
      "      - description_similarity: 0.485\n",
      "      - location_proximity: 0.533\n",
      "      - degree: 1.000\n",
      "      - pagerank: 0.000\n",
      "      - core_number: 0.315\n",
      "   Final Score: 0.476\n",
      "\n",
      "3. Company: Singapore Manufacturing Federation\n",
      "   Job Title: Snr Executive/ Executive, Quality and Safety Standards\n",
      "   Job Type: fulltime\n",
      "   Distance: 8.6 km\n",
      "   Remote: False\n",
      "   Job URL: https://sg.indeed.com/viewjob?jk=01e2366962e4c99b\n",
      "   Direct URL: https://www.mycareersfuture.gov.sg/job/admin/snr-executive-executive-quality-safety-standards-singapore-manufacturing-federation-c9006651ac180c0d5000556360e6fcc1\n",
      "   Scores:\n",
      "      - title_similarity: 0.529\n",
      "      - description_similarity: 0.485\n",
      "      - location_proximity: 0.424\n",
      "      - degree: 1.000\n",
      "      - pagerank: 0.000\n",
      "      - core_number: 0.108\n",
      "   Final Score: 0.444\n",
      "\n",
      "4. Company: Oomph Pte. Ltd.\n",
      "   Job Title: Sales Promoter\n",
      "   Job Type: fulltime\n",
      "   Distance: 8.8 km\n",
      "   Remote: False\n",
      "   Job URL: https://sg.indeed.com/viewjob?jk=000a01db7a6ccc16\n",
      "   Direct URL: https://www.mycareersfuture.gov.sg/job/customer-service/sales-promoter-oomph-e63161cdb7dda2c74dfa04a764ed31af\n",
      "   Scores:\n",
      "      - title_similarity: 0.529\n",
      "      - description_similarity: 0.485\n",
      "      - location_proximity: 0.415\n",
      "      - degree: 1.000\n",
      "      - pagerank: 0.000\n",
      "      - core_number: 0.108\n",
      "   Final Score: 0.442\n",
      "\n",
      "5. Company: PHOENIX OPCO PTE. LTD.\n",
      "   Job Title: Porter\n",
      "   Job Type: fulltime\n",
      "   Distance: 9.0 km\n",
      "   Remote: False\n",
      "   Job URL: https://sg.indeed.com/viewjob?jk=0005a77c5af02f32\n",
      "   Direct URL: https://www.mycareersfuture.gov.sg/job/customer-service/porter-phoenix-opco-c6a4a2faf33919b8d6287a84e39748f9\n",
      "   Scores:\n",
      "      - title_similarity: 0.529\n",
      "      - description_similarity: 0.485\n",
      "      - location_proximity: 0.407\n",
      "      - degree: 0.000\n",
      "      - pagerank: 0.000\n",
      "      - core_number: 0.894\n",
      "   Final Score: 0.380\n"
     ]
    }
   ],
   "source": [
    "get_personalized_recommendations(2, preload_preferences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
