{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import ast\n",
    "from typing import List, Dict, Union, Tuple, Optional\n",
    "from geopy.geocoders import Nominatim\n",
    "import annoy\n",
    "from pathlib import Path\n",
    "import time\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from geopy.distance import geodesic\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle\n",
    "import faiss\n",
    "from dataclasses import dataclass\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "# import ollama\n",
    "\n",
    "# Initialization of job recommendation system components\n",
    "\n",
    "# Define cache paths\n",
    "CACHE_DIR = Path('recommendation_cache')\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "CACHE_PATHS = {\n",
    "    'graph': CACHE_DIR / 'final_complete_graph.pkl',\n",
    "    'annoy_index': CACHE_DIR / 'annoy_index.ann',\n",
    "    'faiss_index': CACHE_DIR / 'faiss_index.pkl', \n",
    "    'graph_metrics': CACHE_DIR / 'graph_metrics.pkl',\n",
    "    'embeddings_norm': CACHE_DIR / 'normalized_embeddings.pkl',\n",
    "    'degree_scores': CACHE_DIR / 'degree_scores.pkl',\n",
    "    'subgraphs': CACHE_DIR / 'subgraphs.pkl',\n",
    "    'model': CACHE_DIR / 'model_cache.pkl',\n",
    "    'node_embeddings': CACHE_DIR / 'node_embeddings.pt'\n",
    "}\n",
    "\n",
    "# Load node embeddings\n",
    "print(\"Loading node embeddings...\")\n",
    "if CACHE_PATHS['node_embeddings'].exists():\n",
    "    print(\"Loading node embeddings from cache...\")\n",
    "    node_embeddings = torch.load(CACHE_PATHS['node_embeddings'])\n",
    "    print(\"Node embeddings loaded successfully!\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Node embeddings file not found. Please ensure node embeddings have been generated and saved from step 8.\")\n",
    "\n",
    "# MultiLabelBinarizer setup for job type decoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([['contract'], ['fulltime'], ['internship'], ['parttime'], ['temporary']])\n",
    "\n",
    "# Geocoder initialization\n",
    "geolocator = Nominatim(user_agent=\"job_recommender_v1\")\n",
    "\n",
    "# Load or initialize model and tokenizer\n",
    "def load_or_init_model():\n",
    "    print(\"Loading/initializing language model...\")\n",
    "    if CACHE_PATHS['model'].exists():\n",
    "        print(\"Loading model from cache...\")\n",
    "        with open(CACHE_PATHS['model'], 'rb') as f:\n",
    "            cache = pickle.load(f)\n",
    "            tokenizer = cache['tokenizer']\n",
    "            model = cache['model']\n",
    "        print(\"Model loaded from cache successfully!\")\n",
    "    else:\n",
    "        print(\"Downloading and caching model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "        model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "        \n",
    "        with open(CACHE_PATHS['model'], 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'tokenizer': tokenizer,\n",
    "                'model': model\n",
    "            }, f)\n",
    "        print(\"Model downloaded and cached successfully!\")\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "    return tokenizer, model, device\n",
    "\n",
    "# Initialize tokenizer, model, and device\n",
    "tokenizer, model, device = load_or_init_model()\n",
    "\n",
    "# Load graph if available\n",
    "if CACHE_PATHS['graph'].exists():\n",
    "    with open(CACHE_PATHS['graph'], 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "else:\n",
    "    print(\"Error: Graph pickle file does not exist. Please check the path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "# Function to build Annoy index\n",
    "def build_ann_index(embeddings_np, n_trees=100):\n",
    "    print(\"\\nBuilding/Loading Annoy index...\")\n",
    "    if CACHE_PATHS['annoy_index'].exists():\n",
    "        print(\"Loading cached Annoy index...\")\n",
    "        index = annoy.AnnoyIndex(embeddings_np.shape[1], 'angular')\n",
    "        index.load(str(CACHE_PATHS['annoy_index']))\n",
    "        print(\"Annoy index loaded successfully!\")\n",
    "        return index\n",
    "    \n",
    "    print(\"Building new Annoy index...\")\n",
    "    index = annoy.AnnoyIndex(embeddings_np.shape[1], 'angular')\n",
    "    \n",
    "    for i in range(len(embeddings_np)):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Adding item {i}/{len(embeddings_np)} to Annoy index...\")\n",
    "        index.add_item(i, embeddings_np[i])\n",
    "    \n",
    "    print(\"Building index with trees...\")\n",
    "    index.build(n_trees)\n",
    "    print(\"Saving index to disk...\")\n",
    "    index.save(str(CACHE_PATHS['annoy_index']))\n",
    "    print(\"Annoy index built and saved successfully!\")\n",
    "    return index\n",
    "\n",
    "# Function to build FAISS index\n",
    "def build_faiss_index(embeddings):\n",
    "    print(\"\\nBuilding/Loading FAISS index...\")\n",
    "    if CACHE_PATHS['faiss_index'].exists():\n",
    "        print(\"Loading cached FAISS index...\")\n",
    "        with open(CACHE_PATHS['faiss_index'], 'rb') as f:\n",
    "            index = pickle.load(f)\n",
    "        print(\"FAISS index loaded successfully!\")\n",
    "        return index\n",
    "            \n",
    "    print(\"Building new FAISS index...\")\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    print(\"Normalizing embeddings...\")\n",
    "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1)[:, np.newaxis]\n",
    "    print(\"Adding vectors to index...\")\n",
    "    index.add(normalized_embeddings.astype('float32'))\n",
    "    \n",
    "    print(\"Saving index to disk...\")\n",
    "    with open(CACHE_PATHS['faiss_index'], 'wb') as f:\n",
    "        pickle.dump(index, f)\n",
    "    print(\"FAISS index built and saved successfully!\")\n",
    "    return index\n",
    "\n",
    "# Function to cache normalized embeddings\n",
    "def cache_normalized_embeddings(embeddings_np):\n",
    "    print(\"\\nPreparing normalized embeddings...\")\n",
    "    if CACHE_PATHS['embeddings_norm'].exists():\n",
    "        print(\"Loading cached normalized embeddings...\")\n",
    "        with open(CACHE_PATHS['embeddings_norm'], 'rb') as f:\n",
    "            normalized = pickle.load(f)\n",
    "        print(\"Normalized embeddings loaded successfully!\")\n",
    "        return normalized\n",
    "    \n",
    "    print(\"Computing normalized embeddings...\")\n",
    "    normalized = embeddings_np / np.linalg.norm(embeddings_np, axis=1)[:, np.newaxis]\n",
    "    print(\"Saving normalized embeddings to disk...\")\n",
    "    with open(CACHE_PATHS['embeddings_norm'], 'wb') as f:\n",
    "        pickle.dump(normalized, f)\n",
    "    print(\"Normalized embeddings cached successfully!\")\n",
    "    return normalized\n",
    "\n",
    "def compute_pagerank_torch(graph, damping=0.85, max_iter=100, tol=1e-6):\n",
    "    print(\"\\nComputing PageRank scores...\")\n",
    "    # Create node ID to index mapping\n",
    "    node_map = {node: idx for idx, node in enumerate(graph.nodes())}\n",
    "    reverse_map = {idx: node for node, idx in node_map.items()}\n",
    "    n = len(node_map)\n",
    "    \n",
    "    # Convert edges using the mapping\n",
    "    edges = [(node_map[e[0]], node_map[e[1]]) for e in graph.edges()]\n",
    "    row = np.array([e[0] for e in edges])\n",
    "    col = np.array([e[1] for e in edges])\n",
    "    data = np.ones(len(edges))\n",
    "    adj_matrix = csr_matrix((data, (row, col)), shape=(n, n))\n",
    "\n",
    "    print(\"Normalizing adjacency matrix...\")\n",
    "    out_degree = np.array(adj_matrix.sum(axis=1)).flatten()\n",
    "    out_degree[out_degree == 0] = 1\n",
    "    D_inv = csr_matrix((1.0 / out_degree, (np.arange(n), np.arange(n))), shape=(n, n))\n",
    "    stochastic_matrix = D_inv @ adj_matrix\n",
    "\n",
    "    print(\"Converting to PyTorch sparse format...\")\n",
    "    coo_matrix = stochastic_matrix.tocoo()\n",
    "    indices = torch.tensor([coo_matrix.row, coo_matrix.col], dtype=torch.long)\n",
    "    values = torch.tensor(coo_matrix.data, dtype=torch.float32)\n",
    "    sparse_matrix = torch.sparse.FloatTensor(indices, values, torch.Size([n, n])).cuda()\n",
    "\n",
    "    print(\"Initializing PageRank computation...\")\n",
    "    pagerank_vector = torch.ones(n, device=\"cuda\") / n\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        new_pagerank_vector = (1 - damping) / n + damping * torch.sparse.mm(sparse_matrix.t(), pagerank_vector.unsqueeze(1)).squeeze()\n",
    "        if torch.norm(new_pagerank_vector - pagerank_vector, p=1) < tol:\n",
    "            print(f\"PageRank converged after {i + 1} iterations\")\n",
    "            break\n",
    "        pagerank_vector = new_pagerank_vector\n",
    "\n",
    "    print(\"PageRank computation completed!\")\n",
    "    # Convert back to original node IDs\n",
    "    pagerank_dict = {reverse_map[i]: score for i, score in enumerate(pagerank_vector.cpu().numpy())}\n",
    "    return pagerank_dict\n",
    "\n",
    "# Function to cache graph metrics\n",
    "def cache_graph_metrics(graph):\n",
    "    print(\"\\nComputing/Loading graph metrics...\")\n",
    "    if CACHE_PATHS['graph_metrics'].exists():\n",
    "        print(\"Loading cached graph metrics...\")\n",
    "        with open(CACHE_PATHS['graph_metrics'], 'rb') as f:\n",
    "            metrics = pickle.load(f)\n",
    "        print(\"Graph metrics loaded successfully!\")\n",
    "        return metrics\n",
    "    \n",
    "    print(\"Computing graph metrics...\")\n",
    "    print(\"Computing degree centrality...\")\n",
    "    degree = dict(graph.degree())\n",
    "    max_degree = max(degree.values())\n",
    "    \n",
    "    print(\"Computing PageRank...\")\n",
    "    pagerank = compute_pagerank_torch(graph)\n",
    "    \n",
    "    print(\"Computing core numbers...\")\n",
    "    core_numbers = nx.core_number(graph)\n",
    "    \n",
    "    metrics = {\n",
    "        'degree': degree,\n",
    "        'max_degree': max_degree,\n",
    "        'pagerank': pagerank,\n",
    "        'core_numbers': core_numbers\n",
    "    }\n",
    "    \n",
    "    print(\"Saving graph metrics to disk...\")\n",
    "    with open(CACHE_PATHS['graph_metrics'], 'wb') as f:\n",
    "        pickle.dump(metrics, f)\n",
    "    print(\"Graph metrics cached successfully!\")\n",
    "    return metrics\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def get_job_description_embedding(text, tokenizer, model, max_chunk_length=512, overlap=50, device='cpu'):\n",
    "    print(\"\\nGenerating job description embedding...\")\n",
    "    tokens_per_chunk = max_chunk_length - 2  \n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i+tokens_per_chunk]) for i in range(0, len(words), tokens_per_chunk - overlap)]\n",
    "    \n",
    "    embeddings = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "        encoded_input = tokenizer(chunk, return_tensors='pt', padding=True, truncation=True, max_length=max_chunk_length).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        \n",
    "        sentence_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embedding = F.normalize(sentence_embedding, p=2, dim=1)\n",
    "        embeddings.append(sentence_embedding.cpu().numpy())\n",
    "\n",
    "    if embeddings:\n",
    "        print(\"Averaging chunk embeddings...\")\n",
    "        return np.mean(embeddings, axis=0).squeeze()\n",
    "    else:\n",
    "        print(\"Warning: No embeddings generated, returning zero vector\")\n",
    "        return np.zeros((384,))\n",
    "\n",
    "def get_job_title_embedding(text, tokenizer, model, device='cpu'):\n",
    "    print(f\"\\nGenerating embedding for job title: {text}\")\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    sentence_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    sentence_embedding = F.normalize(sentence_embedding, p=2, dim=1)\n",
    "    print(\"Job title embedding generated successfully!\")\n",
    "    \n",
    "    return sentence_embedding.cpu().numpy().squeeze()\n",
    "\n",
    "# def process_job_description_with_LLM(document_text):\n",
    "#     \"\"\"Process a single document using the same LLM setup.\"\"\"\n",
    "#     model_name = 'capybarahermes-2.5-mistral-7b.Q5_K_M.gguf:latest'\n",
    "#     prompt = f\"\"\"You are an expert in understanding job descriptions and extracting the details and even nuanced requirements for the job. Your goal is to read the input slowly and take time to consider what is written, extract the information and break it down into these 3 aspects:\n",
    "#     1. responsibilites \n",
    "#     2. qualifications\n",
    "#     3. skills, technical and non-technical\n",
    "# and summarize it in point form line by line.\n",
    "# With each aspect answered, ensure that each of the aspects are properly differentiated and avoid overlaps as much as possible.\"\"\"\n",
    "    \n",
    "#     try:\n",
    "#         response = ollama.chat(\n",
    "#             model=model_name,\n",
    "#             messages=[\n",
    "#                 {'role': 'system', 'content': prompt},\n",
    "#                 {'role': 'user', 'content': document_text}\n",
    "#             ]\n",
    "#         )\n",
    "#         response_text = response['message']['content']\n",
    "        \n",
    "#         # Clean the response text\n",
    "#         # Remove special characters except alphanumeric, spaces, periods and commas\n",
    "#         cleaned_text = re.sub(r'[^A-Za-z0-9\\s.,]', '', response_text)\n",
    "#         # Remove point formatted numbers but keep time patterns\n",
    "#         cleaned_text = re.sub(r'(?<!\\d)(\\d+)\\.(?!\\d)', '', cleaned_text).strip()\n",
    "        \n",
    "#         return cleaned_text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing document: {e}\")\n",
    "#         return None\n",
    "\n",
    "@dataclass\n",
    "class UserPreferences:\n",
    "    \"\"\"User preferences for job recommendations\"\"\"\n",
    "    location: Optional[Tuple[float, float]] = None\n",
    "    location_name: Optional[str] = None  # Added for geocoding\n",
    "    job_title: Optional[str] = None\n",
    "    job_description: Optional[str] = None\n",
    "    max_distance_km: float = 10.0\n",
    "    weights: dict = None\n",
    "    remote_preference: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.weights is None:\n",
    "            self.weights = {\n",
    "                'title_similarity': 0.3,\n",
    "                'description_similarity': 0.2,\n",
    "                'location_proximity': 0.2,\n",
    "                'degree': 0.1,\n",
    "                'pagerank': 0.1,\n",
    "                'core_number': 0.1\n",
    "            }\n",
    "\n",
    "def get_graph_based_recommendations(\n",
    "    graph: nx.Graph,\n",
    "    node_embeddings: torch.Tensor,\n",
    "    preferences: UserPreferences,\n",
    "    n_hops: int = 2,\n",
    "    top_k: int = 5,\n",
    "    n_candidates: int = 1000\n",
    ") -> List[dict]:\n",
    "    \"\"\"Generate recommendations using both FAISS and Annoy for hybrid search\"\"\"\n",
    "    print(\"\\nStarting recommendation generation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Loading job URL data...\")\n",
    "    df = pd.read_pickle('./recommendation_cache/final_complete_graph_dataframe.pkl')\n",
    "    \n",
    "    print(\"Preparing embeddings...\")\n",
    "    embeddings_np = node_embeddings.numpy()\n",
    "    print(f\"Embeddings shape: {embeddings_np.shape}\")\n",
    "    \n",
    "    print(\"\\nInitializing search indices...\")\n",
    "    faiss_index = build_faiss_index(embeddings_np)\n",
    "    annoy_index = build_ann_index(embeddings_np)\n",
    "    normalized_embeddings = cache_normalized_embeddings(embeddings_np)\n",
    "    graph_metrics = cache_graph_metrics(graph)\n",
    "    \n",
    "    print(\"\\nGenerating candidate nodes...\")\n",
    "    candidate_nodes = set()\n",
    "    if preferences.job_title:\n",
    "        print(f\"Finding similar jobs to title: {preferences.job_title}\")\n",
    "        title_embedding = get_job_title_embedding(preferences.job_title, tokenizer, model, device)\n",
    "        title_embedding = title_embedding / np.linalg.norm(title_embedding)\n",
    "        \n",
    "        print(\"Searching with FAISS...\")\n",
    "        D_faiss, I_faiss = faiss_index.search(title_embedding.astype('float32').reshape(1,-1), top_k)\n",
    "        faiss_candidates = {f\"job_{idx}\" for idx in I_faiss[0]}\n",
    "        \n",
    "        print(\"Searching with Annoy...\")\n",
    "        annoy_candidates = {f\"job_{idx}\" for idx in annoy_index.get_nns_by_vector(\n",
    "            title_embedding, \n",
    "            top_k,\n",
    "            search_k=-1\n",
    "        )}\n",
    "        \n",
    "        candidate_nodes = faiss_candidates.union(annoy_candidates)\n",
    "        print(f\"Found {len(candidate_nodes)} candidate nodes\")\n",
    "    else:\n",
    "        print(\"No job title specified, using PageRank for candidate selection...\")\n",
    "        weights = np.array([graph_metrics['pagerank'][node] for node in graph.nodes()])\n",
    "        weights = weights / weights.sum()\n",
    "        candidate_nodes = set(np.random.choice(\n",
    "            list(graph.nodes()),\n",
    "            size=min(n_candidates, len(graph)),\n",
    "            p=weights,\n",
    "            replace=False\n",
    "        ))\n",
    "    \n",
    "    print(\"\\nComputing job scores...\")\n",
    "    subgraph = graph.subgraph(candidate_nodes)\n",
    "    degree_scores = dict(subgraph.degree())\n",
    "    max_degree = max(degree_scores.values()) if degree_scores else 1\n",
    "    \n",
    "    batch_size = 100\n",
    "    job_scores = []\n",
    "    total_batches = len(candidate_nodes) // batch_size + (1 if len(candidate_nodes) % batch_size else 0)\n",
    "    \n",
    "    for i in range(0, len(candidate_nodes), batch_size):\n",
    "        batch_nodes = list(candidate_nodes)[i:i + batch_size]\n",
    "        current_batch = i // batch_size + 1\n",
    "        print(f\"\\nProcessing batch {current_batch}/{total_batches}...\")\n",
    "        batch_scores = []\n",
    "        \n",
    "        for node in batch_nodes:\n",
    "            try:\n",
    "                node_id = node  # Keep the full node ID (e.g. 'job_0')\n",
    "                attrs = graph.nodes[node]\n",
    "                \n",
    "                score_components = {}\n",
    "                \n",
    "                if preferences.job_title:\n",
    "                    score_components['title_similarity'] = np.dot(\n",
    "                        title_embedding,\n",
    "                        normalized_embeddings[int(node.split('_')[1])]  # Extract number from 'job_X'\n",
    "                    )\n",
    "                if preferences.job_description:\n",
    "                    desc_embedding = get_job_description_embedding(\n",
    "                        preferences.job_description,\n",
    "                        tokenizer,\n",
    "                        model,\n",
    "                        device=device\n",
    "                    )\n",
    "                    score_components['description_similarity'] = np.dot(\n",
    "                        desc_embedding,\n",
    "                        normalized_embeddings[int(node.split('_')[1])]  # Extract number from 'job_X'\n",
    "                    )\n",
    "                if preferences.location:\n",
    "                    job_location = (\n",
    "                        ast.literal_eval(attrs['lat_long'])\n",
    "                        if isinstance(attrs['lat_long'], str)\n",
    "                        else attrs['lat_long']\n",
    "                    )\n",
    "                    distance = geodesic(preferences.location, job_location).kilometers\n",
    "                    score_components['location_proximity'] = max(0, 1 - (distance / preferences.max_distance_km))\n",
    "                \n",
    "                score_components['degree'] = degree_scores[node] / max_degree\n",
    "                score_components['pagerank'] = graph_metrics['pagerank'][node]\n",
    "                score_components['core_number'] = graph_metrics['core_numbers'][node] / max(graph_metrics['core_numbers'].values())\n",
    "                \n",
    "                final_score = sum(\n",
    "                    score * preferences.weights.get(component, 0.1)\n",
    "                    for component, score in score_components.items()\n",
    "                )\n",
    "                \n",
    "                job_type_encoded = attrs['job_type_encoding']\n",
    "                if isinstance(job_type_encoded, str):\n",
    "                    job_type_encoded = ast.literal_eval(job_type_encoded)\n",
    "                job_type_encoded = np.array(job_type_encoded)\n",
    "                if job_type_encoded.ndim == 1:\n",
    "                    job_type_encoded = job_type_encoded.reshape(1, -1)\n",
    "\n",
    "                job_type_decoded = mlb.inverse_transform(job_type_encoded)[0]\n",
    "\n",
    "                batch_scores.append({\n",
    "                    'node_id': node,\n",
    "                    'job_id': node,  # Keep the full node ID (e.g. 'job_0')\n",
    "                    'company': attrs['company'],\n",
    "                    'job_type': job_type_decoded,\n",
    "                    'location': attrs['lat_long'],\n",
    "                    'is_remote': attrs['is_remote'],\n",
    "                    'score_components': score_components,\n",
    "                    'final_score': final_score\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing node {node}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        job_scores.extend(batch_scores)\n",
    "    \n",
    "    print(\"\\nSorting recommendations...\")\n",
    "    recommendations = sorted(\n",
    "        job_scores,\n",
    "        key=lambda x: x['final_score'],\n",
    "        reverse=True\n",
    "    )[:top_k]\n",
    "    \n",
    "    # Get URLs and titles for only the top recommendations\n",
    "    for rec in recommendations:\n",
    "        job_data = df[df['node_id'] == rec['job_id']].iloc[0]\n",
    "        rec['job_url'] = job_data['job_url']\n",
    "        rec['job_url_direct'] = job_data['job_url_direct']\n",
    "        rec['title'] = job_data['title']  # Add job title to recommendations\n",
    "    \n",
    "    print(f\"\\nRecommendation generation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    return recommendations\n",
    "\n",
    "def get_personalized_recommendations(preload_preferences=False):\n",
    "    \"\"\"Interactive function to get user preferences and return recommendations\"\"\"\n",
    "    print(\"\\nJob Recommendation System\")\n",
    "    print(\"------------------------\")\n",
    "    \n",
    "    preferences = UserPreferences()\n",
    "    \n",
    "    if preload_preferences:\n",
    "        # Preload default preferences\n",
    "        preferences.location = (1.3521, 103.8198)  # Singapore coordinates\n",
    "        preferences.location_name = \"Singapore\"\n",
    "        preferences.max_distance_km = 10\n",
    "        preferences.job_title = \"Service Technician\"\n",
    "        preferences.job_description = \"Skilled in mechanical work, always hardworking, able to handle odd hours\"\n",
    "        preferences.weights = {\n",
    "            'title_similarity': 0.3,\n",
    "            'description_similarity': 0.2,\n",
    "            'location_proximity': 0.2,\n",
    "            'degree': 0.1,\n",
    "            'pagerank': 0.1,\n",
    "            'core_number': 0.1\n",
    "        }\n",
    "        print(\"Using preloaded preferences:\")\n",
    "        print(f\"Location: {preferences.location_name}\")\n",
    "        print(f\"Job Title: {preferences.job_title}\")\n",
    "        print(f\"Job Description: {preferences.job_description}\")\n",
    "        print(f\"Max Distance: {preferences.max_distance_km} km\")\n",
    "        print(\"Weights:\", preferences.weights)\n",
    "    else:\n",
    "        use_location = input(\"Would you like to specify a location? (y/n): \").lower() == 'y'\n",
    "        if use_location:\n",
    "            location_input = int(input(\"Enter Postal Code (e.g., 123456): \"))\n",
    "            try:\n",
    "                print(\"Geocoding location...\")\n",
    "                location = geolocator.geocode(location_input)\n",
    "                if location:\n",
    "                    preferences.location = (location.latitude, location.longitude)\n",
    "                    preferences.location_name = location_input\n",
    "                    print(f\"Location found: {location.address}\")\n",
    "                else:\n",
    "                    print(\"Location not found. Please enter coordinates manually.\")\n",
    "                    lat = float(input(\"Enter latitude (e.g., 1.3521 for Singapore): \"))\n",
    "                    lon = float(input(\"Enter longitude (e.g., 103.8198): \"))\n",
    "                    preferences.location = (lat, lon)\n",
    "            except Exception as e:\n",
    "                print(f\"Error geocoding location: {str(e)}\")\n",
    "                print(\"Please enter coordinates manually.\")\n",
    "                lat = float(input(\"Enter latitude (e.g., 1.3521 for Singapore): \"))\n",
    "                lon = float(input(\"Enter longitude (e.g., 103.8198): \"))\n",
    "                preferences.location = (lat, lon)\n",
    "            \n",
    "            preferences.max_distance_km = float(input(\"Maximum distance in km (default 10): \") or 10)\n",
    "        \n",
    "        use_title = input(\"Would you like to specify a job title? (y/n): \").lower() == 'y'\n",
    "        if use_title:\n",
    "            preferences.job_title = input(\"Enter job title: \")\n",
    "        \n",
    "        use_desc = input(\"Would you like to specify a job description? (y/n): \").lower() == 'y'\n",
    "        if use_desc:\n",
    "            preferences.job_description = input(\"Enter job description: \")\n",
    "        \n",
    "        print(\"\\nSet the importance for each of the following factors on a scale from 0 (not important) to 1 (very important):\")\n",
    "        if use_title:\n",
    "            preferences.weights['title_similarity'] = float(input(\"How important is it for the job title to match your preferences? (default 0.3): \") or 0.3)\n",
    "        if use_desc:\n",
    "            preferences.weights['description_similarity'] = float(input(\"How important is it for the job description to match your skills and experience? (default 0.2): \") or 0.2)\n",
    "        if use_location:\n",
    "            preferences.weights['location_proximity'] = float(input(\"How important is it for the job to be near your preferred location? (default 0.2): \") or 0.2)\n",
    "\n",
    "        preferences.weights['degree'] = float(input(\"How important is it for the job to be popular or well-connected within the network? (default 0.1): \") or 0.1)\n",
    "        preferences.weights['pagerank'] = float(input(\"How important is it for the job to be influential within the network? (default 0.1): \") or 0.1)\n",
    "        preferences.weights['core_number'] = float(input(\"How important is it for the job to be well-connected within its area or community? (default 0.1): \") or 0.1)\n",
    "    \n",
    "    print(\"\\nGenerating recommendations...\")\n",
    "    recommendations = get_graph_based_recommendations(\n",
    "        graph=graph,\n",
    "        node_embeddings=node_embeddings,\n",
    "        preferences=preferences\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTop Recommendations:\")\n",
    "    print(\"-------------------\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"\\n{i}. Company: {rec['company']}\")\n",
    "        print(f\"   Job Title: {rec['title']}\")  # Added job title to output\n",
    "        print(f\"   Job Type: {', '.join(rec['job_type'])}\")\n",
    "        if preferences.location:\n",
    "            job_loc = ast.literal_eval(rec['location']) if isinstance(rec['location'], str) else rec['location']\n",
    "            distance = geodesic(preferences.location, job_loc).kilometers\n",
    "            print(f\"   Distance: {distance:.1f} km\")\n",
    "        print(f\"   Remote: {rec['is_remote']}\")\n",
    "        print(f\"   Job URL: {rec['job_url']}\")\n",
    "        print(f\"   Direct URL: {rec['job_url_direct']}\")\n",
    "        print(\"   Scores:\")\n",
    "        for component, score in rec['score_components'].items():\n",
    "            print(f\"      - {component}: {score:.3f}\")\n",
    "        print(f\"   Final Score: {rec['final_score']:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
